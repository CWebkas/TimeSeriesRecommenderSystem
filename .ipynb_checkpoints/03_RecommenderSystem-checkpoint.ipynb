{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "pacman::p_load(tidyverse, mltools)\n",
    "library(M4metalearning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Import required data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load(\"~/data/SPDat_reduced_all.RData\")\n",
    "load(\"~/data/spdat_reduced_cls.RData\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(2021)\n",
    "indices <- sample(length(spdat_reduced_all))\n",
    "\n",
    "\n",
    "to_train <- length(indices)*0.8\n",
    "from_test <- length(indices)*0.8+1\n",
    "\n",
    "\n",
    "\n",
    "SPdata_train <- spdat_reduced_all[indices[1:to_train]]\n",
    "\n",
    "SPdata_test <-spdat_reduced_all[indices[from_test:length(indices)]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# 1. Standard Model runthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "comment": false,
    "lines_to_next_cell": 0,
    "warning": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in if (class(newdata) != \"xgb.DMatrix\") newdata <- xgb.DMatrix(newdata, :\n",
      "\"Bedingung hat L채nge > 1 und nur das erste Element wird benutzt\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Classification error:  0.4312\"\n",
      "[1] \"Selected OWI :  0.8806\"\n",
      "[1] \"Weighted OWI :  1.5709\"\n",
      "[1] \"Naive Weighted OWI :  1.7167\"\n",
      "[1] \"Oracle OWI:  0.8213\"\n",
      "[1] \"Single method OWI:  1\"\n",
      "[1] \"Average OWI:  1.574\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "SPdata_train <- calc_errors(SPdata_train)\n",
    "\n",
    "\n",
    "train_data <- create_feat_classif_problem(SPdata_train)\n",
    "\n",
    "\n",
    "# params are left default except for parallelization\n",
    "        param <- list(max_depth=14, eta=0.575188, nthread = 11, silent=1,\n",
    "                  objective=error_softmax_obj,\n",
    "                  num_class=ncol(train_data$errors),\n",
    "                  subsample=0.9161483,\n",
    "                  colsample_bytree=0.7670739\n",
    "    )\n",
    "\n",
    "\n",
    "set.seed(1345) #set the seed because xgboost is random!\n",
    "meta_model <- train_selection_ensemble(train_data$data, train_data$errors)\n",
    "\n",
    "\n",
    "\n",
    "# In order to create the newdata matrix required, the function create_feat_classif_problem can be used, it just produces the data object, not errors and labels.\n",
    "test_data <- create_feat_classif_problem(SPdata_test)\n",
    "\n",
    "\n",
    "\n",
    "# predict takes as parameters the model and a matrix with the features of the series. It outputs the predictions of the metalearning model, a matrix with the weights of the linear combination of methods, one row for each series.\n",
    "\n",
    "preds <- predict_selection_ensemble(meta_model, test_data$data)\n",
    "\n",
    "\n",
    "# The last step is calculating the actual forecasts by the linear combinations produced by the metalearning model.\n",
    "tstdat <- ensemble_forecast(preds, SPdata_test)\n",
    "\n",
    "summary <- summary_performance(preds, dataset = tstdat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# 2 Run the pipeline using optimized parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "comment": false,
    "warning": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in if (class(newdata) != \"xgb.DMatrix\") newdata <- xgb.DMatrix(newdata, :\n",
      "\"Bedingung hat L채nge > 1 und nur das erste Element wird benutzt\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Classification error:  0.443\"\n",
      "[1] \"Selected OWI :  0.8804\"\n",
      "[1] \"Weighted OWI :  1.5713\"\n",
      "[1] \"Naive Weighted OWI :  1.7167\"\n",
      "[1] \"Oracle OWI:  0.8213\"\n",
      "[1] \"Single method OWI:  1\"\n",
      "[1] \"Average OWI:  1.574\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate forecast errors for each method\n",
    "SPdata_train <- calc_errors(SPdata_train)\n",
    "\n",
    "# Rank methods based on softmax logic\n",
    "train_data <- create_feat_classif_problem(SPdata_train)\n",
    "\n",
    "\n",
    "# set the parameters based on optimization runthrough\n",
    "    param <- list(max_depth=11, eta=0.5543024, nthread = 11, silent=1,\n",
    "                  objective=error_softmax_obj,\n",
    "                  num_class=ncol(train_data$errors),\n",
    "                  subsample=0.9532598,\n",
    "                  colsample_bytree=1\n",
    "    )\n",
    "\n",
    "\n",
    "set.seed(1345) #set the seed because xgboost is random\n",
    "\n",
    "# Train\n",
    "meta_model <- train_selection_ensemble(train_data$data, train_data$errors, param=param)\n",
    "\n",
    "\n",
    "\n",
    "# In order to create the newdata matrix required, the function create_feat_classif_problem can be used, it just produces the data object, not errors and labels.\n",
    "test_data <- create_feat_classif_problem(SPdata_test)\n",
    "\n",
    "\n",
    "\n",
    "# predict takes as parameters the model and a matrix with the features of the series. It outputs the predictions of the metalearning model, a matrix with the weights of the linear combination of methods, one row for each series.\n",
    "\n",
    "preds <- predict_selection_ensemble(meta_model, test_data$data)\n",
    "\n",
    "\n",
    "# The last step is calculating the actual forecasts by the linear combinations produced by the metalearning model.\n",
    "tstdat <- ensemble_forecast(preds, SPdata_test)\n",
    "\n",
    "summary <- summary_performance(preds, dataset = tstdat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Hyperparameter Tuning (will run multiple hours!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "pms = hyperparameter_search(SPdata_train, filename = 'find_hyper.RData', n_iter = 500, n.cores=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# 3. All Cluster labels\n",
    "\n",
    "### Import Cluster-labelled Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "load(\"~/data/spdat_reduced_cls.RData\")\n",
    "\n",
    "sp_cls <- spdat_reduced_all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Save cls_label as only feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for(i in 1:length(sp_cls)){\n",
    "  sp_cls[[i]]$features <- sp_cls[[i]]$cls_info[,\"cls_label\"]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### One hot encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cls_dat <- c()\n",
    "for(i in 1:length(sp_cls)){\n",
    "  cls_dat[length(cls_dat)+1]<- sp_cls[[i]]$features\n",
    "}\n",
    "\n",
    "cls_dat <- data.table::data.table(cls=as_factor(unlist(cls_dat)))\n",
    "\n",
    "\n",
    "dt <- one_hot(cls_dat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Save as one hot encoded feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "for(i in 1:length(sp_cls)){\n",
    "  sp_cls[[i]]$features <- dt[i,]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test SPlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "set.seed(2021)\n",
    "indices <- sample(length(sp_cls))\n",
    "\n",
    "\n",
    "to_train <- length(indices)*0.8\n",
    "from_test <- length(indices)*0.8+1\n",
    "\n",
    "\n",
    "SPdata_train <- sp_cls[indices[1:to_train]]\n",
    "\n",
    "SPdata_test <-sp_cls[indices[from_test:length(indices)]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Run Hyndman Pipeline with the single one hot encoded cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "comment": false,
    "lines_to_next_cell": 2,
    "warning": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in if (class(newdata) != \"xgb.DMatrix\") newdata <- xgb.DMatrix(newdata, :\n",
      "\"Bedingung hat L채nge > 1 und nur das erste Element wird benutzt\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Classification error:  0.5726\"\n",
      "[1] \"Selected OWI :  0.938\"\n",
      "[1] \"Weighted OWI :  1.6132\"\n",
      "[1] \"Naive Weighted OWI :  1.7167\"\n",
      "[1] \"Oracle OWI:  0.8213\"\n",
      "[1] \"Single method OWI:  1\"\n",
      "[1] \"Average OWI:  1.574\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "SPdata_train <- calc_errors(SPdata_train)\n",
    "\n",
    "\n",
    "train_data <- create_feat_classif_problem(SPdata_train)\n",
    "\n",
    "\n",
    "# set the parameters based on optimization runthrough\n",
    "    param <- list(max_depth=14, eta=0.575188, nthread = 11, silent=1,\n",
    "                  objective=error_softmax_obj,\n",
    "                  num_class=ncol(train_data$errors),\n",
    "                  subsample=0.9161483,\n",
    "                  colsample_bytree=0.7670739\n",
    "    )\n",
    "\n",
    "\n",
    "set.seed(1345) #set the seed because xgboost is random!\n",
    "meta_model <- train_selection_ensemble(train_data$data, train_data$errors, param=param)\n",
    "\n",
    "\n",
    "\n",
    "# In order to create the newdata matrix required, the function create_feat_classif_problem can be used, it just produces the data object, not errors and labels.\n",
    "test_data <- create_feat_classif_problem(SPdata_test)\n",
    "\n",
    "\n",
    "\n",
    "# predict takes as parameters the model and a matrix with the features of the series. It outputs the predictions of the metalearning model, a matrix with the weights of the linear combination of methods, one row for each series.\n",
    "\n",
    "preds <- predict_selection_ensemble(meta_model, test_data$data)\n",
    "\n",
    "\n",
    "# The last step is calculating the actual forecasts by the linear combinations produced by the metalearning model.\n",
    "tstdat <- ensemble_forecast(preds, SPdata_test)\n",
    "\n",
    "summary <- summary_performance(preds, dataset = tstdat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Only Sampled Cluster data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "sp_cls <- c()\n",
    "\n",
    "for (i in 1:length(spdat_reduced_all)){\n",
    "  if(spdat_reduced_all[[i]]$cls_information$insamp == 1 ){\n",
    "  sp_cls[[length(sp_cls)+1]] <- spdat_reduced_all[[i]]}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save cls_label as only feature\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "for(i in 1:length(sp_cls)){\n",
    "  sp_cls[[i]]$features <- sp_cls[[i]]$cls_info[,\"cls_label\"]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### One hot encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Joining, by = c(\"cls\", \"rownum\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cls_dat <- c()\n",
    "for(i in 1:length(sp_cls)){\n",
    "  cls_dat[length(cls_dat)+1]<- sp_cls[[i]]$features\n",
    "}\n",
    "\n",
    "cls_dat <- data.table::data.table(cls=as_factor(unlist(cls_dat)))\n",
    "\n",
    "\n",
    "\n",
    "set.seed(2020)\n",
    "randsamps <-cls_dat %>% mutate(rownum=1:nrow(cls_dat)) %>%  group_by(cls) %>% sample_n(1)\n",
    "unsamped <- cls_dat %>% mutate(rownum=1:nrow(cls_dat)) %>% anti_join(randsamps)\n",
    "\n",
    "dt <- one_hot(cls_dat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Save as one hot encoded feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "for(i in 1:length(sp_cls)){\n",
    "  sp_cls[[i]]$features <- dt[i,]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test SPlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(2021)\n",
    "indices <- sample(length(sp_cls))\n",
    "\n",
    "\n",
    "to_train <- length(indices)*0.8\n",
    "from_test <- length(indices)*0.8+1\n",
    "\n",
    "SPdata_train <- sp_cls[indices[1:to_train]]\n",
    "\n",
    "SPdata_test <-sp_cls[indices[from_test:length(indices)]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Run Hyndman Pipeline with the single one hot encoded cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in if (class(newdata) != \"xgb.DMatrix\") newdata <- xgb.DMatrix(newdata, :\n",
      "\"Bedingung hat L채nge > 1 und nur das erste Element wird benutzt\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Classification error:  0.5086\"\n",
      "[1] \"Selected OWI :  0.8767\"\n",
      "[1] \"Weighted OWI :  1.6116\"\n",
      "[1] \"Naive Weighted OWI :  1.6311\"\n",
      "[1] \"Oracle OWI:  0.7175\"\n",
      "[1] \"Single method OWI:  0.92\"\n",
      "[1] \"Average OWI:  1.384\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "SPdata_train <- calc_errors(SPdata_train)\n",
    "\n",
    "\n",
    "train_data <- create_feat_classif_problem(SPdata_train)\n",
    "\n",
    "\n",
    "# set the parameters based on optimization runthrough\n",
    "    param <- list(max_depth=14, eta=0.575188, nthread = 11, silent=1,\n",
    "                  objective=error_softmax_obj,\n",
    "                  num_class=ncol(train_data$errors),\n",
    "                  subsample=0.9161483,\n",
    "                  colsample_bytree=0.7670739\n",
    "    )\n",
    "\n",
    "\n",
    "set.seed(1345) #set the seed because xgboost is random!\n",
    "meta_model <- train_selection_ensemble(train_data$data, train_data$errors, param=param)\n",
    "\n",
    "\n",
    "\n",
    "# In order to create the newdata matrix required, the function create_feat_classif_problem can be used, it just produces the data object, not errors and labels.\n",
    "test_data <- create_feat_classif_problem(SPdata_test)\n",
    "\n",
    "\n",
    "\n",
    "# predict takes as parameters the model and a matrix with the features of the series. It outputs the predictions of the metalearning model, a matrix with the weights of the linear combination of methods, one row for each series.\n",
    "\n",
    "preds <- predict_selection_ensemble(meta_model, test_data$data)\n",
    "\n",
    "\n",
    "# The last step is calculating the actual forecasts by the linear combinations produced by the metalearning model.\n",
    "tstdat <- ensemble_forecast(preds, SPdata_test)\n",
    "\n",
    "summary <- summary_performance(preds, dataset = tstdat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Model-Based feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAANlBMVEUAAAAXFxcqKio8PDxN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\ngg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mat <- xgboost::xgb.importance (feature_names = colnames(test_data$data),\n",
    "                       model = meta_model)\n",
    "xgboost::xgb.plot.importance(importance_matrix = mat[1:15], cex=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Closer examination of single features among their quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "tstfeats <- lapply(1:6408, function(i){SPdata_test[[i]]$features}) %>% bind_rows(.id = \"datnum\")\n",
    "\n",
    "\n",
    "\n",
    "featquantile <- quantile(tstfeats$Y__index_mass_quantile__q_0.9, probs = seq(0,1,0.1))\n",
    "\n",
    "\n",
    "# which elements in tstfeats are in which feature quantiles?\n",
    "\n",
    "featquantlst <- lapply(1:10,function(i){tstfeats[tstfeats$Y__index_mass_quantile__q_0.9>= featquantile[i]&\n",
    "           tstfeats$Y__index_mass_quantile__q_0.9< featquantile[i+1],]})\n",
    "\n",
    "# provide labels\n",
    "featquantlst <- lapply(1:10, function(i){featquantlst[[i]] %>% mutate(featquant = i)})\n",
    "\n",
    "# merge together\n",
    "featquantdf <- featquantlst %>% bind_rows() %>% mutate(datnum=as.numeric(datnum))\n",
    "\n",
    "\n",
    "# using datnum, add featquant to entries in preds\n",
    "\n",
    "predsdf <- as_data_frame(preds) \n",
    "\n",
    "names(predsdf)<- names(data.frame(train_data[[\"errors\"]]))\n",
    "\n",
    "\n",
    "datqtl <- predsdf %>% mutate(datnum = c(seq(1, 6408))) %>% left_join(featquantdf %>% select(datnum, featquant)) %>% group_by(featquant) %>% summarise_at(vars(-group_cols(), -datnum), mean) %>% rename() \n",
    "\n",
    "\n",
    "quantdat_ready <- reshape2::melt(datqtl, measure.vars = c(names(predsdf)))\n",
    "\n",
    "\n",
    "p1 <- ggplot(quantdat_ready, aes(fill=variable, y=value, x=featquant))+geom_bar(position = 'stack', stat='identity')+ggtitle(\"Index_mass_quantile_q_0.9\")+labs(x='Feature Quantile', fill=\"Forecast\", y=\"Average Probability\")+scale_fill_manual(values =c(\"#CC79A7\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F0E442\", \"#0072B2\", \"#D55E00\",\"#999999\"))+scale_x_discrete(limits=c(1,2,3,4,5,6,7,8,9,10))\n",
    "\n",
    "\n",
    "# can be continued for less important features as well\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Feature Importance over entire model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\n",
    "colours = c(\"1\" = \"#62879c\", \"2\" = \"#FF9900\", \"3\" = \"#CCFF00\", \"4\" = \"#fff478\", \"6\" = \"#00FF66\", \"7\"=\"#00FFFF\", \"8\"=\"#FF0000\", \"9\"=\"#3300FF\", \"5\"=\"#CC00FF\", \"0\"=\"#000000\")\n",
    "\n",
    "plt_allfeatimp <- mat %>% head(15) %>% rename(names = Feature) %>% \n",
    "  left_join(feat_importance_clustered %>% select(-importances), by='names') %>% \n",
    "  mutate(cls = as_factor(cls), names = gsub('_','.', names)) %>% \n",
    "  ggplot(aes(y=Gain, x=reorder(names, Gain), fill=cls))+geom_bar(stat='identity', position = 'dodge')+coord_flip()+\n",
    "  theme(axis.title.y = element_blank())+\n",
    "  scale_fill_manual(values=colours)\n",
    "\n",
    "plt_allfeatimp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Probabilities for forecasting methods\n",
    "\n",
    "### Max Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_proba <- tibble(\n",
    "  max_probability = factor(max.col(preds), \n",
    "  labels = names(data.frame(train_data[[\"errors\"]])))) %>% group_by(max_probability) %>% tally() %>% \n",
    "  rename(method.name = max_probability, count = n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Mean Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "mean_proba <- tibble(\n",
    "  method_name = names(data.frame(train_data[[\"errors\"]])), \n",
    "  column_avg = colSums(preds)/nrow(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "mean_proba <- tibble(\n",
    "  method_name = names(data.frame(train_data[[\"errors\"]])), \n",
    "  column_avg = colSums(preds)/nrow(preds))\n",
    "\n",
    "mean_proba <- mean_proba %>% mutate(method_name = gsub('_','.', method_name)) %>% rename(columnAverage = column_avg)\n",
    "\n",
    "\n",
    "# plotting the share of maximum for each method\n",
    "\n",
    "\n",
    "max_proba <- max_proba %>% mutate(method.name = gsub('_', '.', method.name))\n",
    "\n",
    "maxplt <- ggplot(max_proba,aes(x=method.name, y=count))+\n",
    "#  scale_x_discrete(limits=mean_proba$method_name)+\n",
    "  geom_bar(fill = c(\"#CC79A7\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F0E442\", \"#0072B2\", \"#D55E00\",\"#999999\"), stat='identity')+\n",
    "  scale_x_discrete(limits=max_proba$method.name)+\n",
    "  ggtitle('Largest probabilities')+\n",
    "    theme(\n",
    "    axis.text.x = element_text(angle = 90),\n",
    "    axis.title.x = element_blank(),\n",
    "    title = element_text(size=12))\n",
    "\n",
    "# plotting the average probability for each method\n",
    "\n",
    "\n",
    "\n",
    "meanplt <-ggplot(mean_proba, aes(x=method_name, y=columnAverage))+geom_bar(fill = c(\"#CC79A7\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F0E442\", \"#0072B2\", \"#D55E00\",\"#999999\"),stat='identity')+\n",
    "  scale_x_discrete(limits=mean_proba$method_name)+\n",
    "  ggtitle('Average probabilities')+  \n",
    "  theme(\n",
    "    axis.text.x = element_text(angle = 90),\n",
    "    axis.title.x = element_blank(),\n",
    "    title = element_text(size=12))\n",
    "\n",
    "maxplt\n",
    "meanplt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Cluster-Specific Max Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_max_importance_plts <- lapply(0:39, function(i, colours =  c(\"#CC79A7\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F0E442\", \"#0072B2\", \"#D55E00\",\"#999999\")){\n",
    "  cls_indxs <- labs %>% \n",
    "  filter(cls_lab == i) %>% \n",
    "  select(tstdat_indxno) %>% \n",
    "  unlist()\n",
    "\n",
    "# change prediction matrix to df\n",
    "predsdf <- data.frame(preds)\n",
    "\n",
    "# renaming prediction df to our methods\n",
    "names(predsdf) <- names(data.frame(train_data[[\"errors\"]]))\n",
    "\n",
    "# selecting rows of interest\n",
    "sub_tstdat <- predsdf[cls_indxs,]\n",
    "\n",
    "\n",
    "\n",
    "max_proba <- tibble(max_probability = factor(max.col(sub_tstdat), ))\n",
    "\n",
    "# selector tool for relevant algorithms\n",
    "apparent_algos <- sort(unlist(unique(max_proba$max_probability)))\n",
    "\n",
    "# plotting the share of maximum for each method\n",
    "ggplot(max_proba, \n",
    "       aes(max_probability))+\n",
    "  geom_bar(aes(y =(..count..)/sum(..count..)), fill=colours[apparent_algos])+\n",
    "  scale_fill_manual(values = c('orange', 'green'))+\n",
    "  ggtitle(paste('Share of most probable methods in cls ', i))+\n",
    "  scale_x_discrete(labels = names(sub_tstdat)[apparent_algos])+\n",
    "  theme(\n",
    "    axis.text.x = element_text(angle = 90, size=20),\n",
    "    title = element_text(size=25))\n",
    "\n",
    "\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Cluster-Specific Mean importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cls_mean_importance_plts <- lapply(0:39, function(i, colours = c(\"#CC79A7\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F0E442\", \"#0072B2\", \"#D55E00\",\"#999999\")){\n",
    "  cls_indxs <- labs %>% \n",
    "  filter(cls_lab == i) %>% \n",
    "  select(tstdat_indxno) %>% \n",
    "  unlist()\n",
    "\n",
    "# change prediction matrix to df\n",
    "predsdf <- data.frame(preds)\n",
    "\n",
    "# renaming prediction df to our methods\n",
    "names(predsdf) <- names(data.frame(train_data[[\"errors\"]]))\n",
    "\n",
    "# selecting rows of interest\n",
    "sub_tstdat <- predsdf[cls_indxs,]\n",
    "\n",
    "\n",
    "\n",
    "mean_proba <- tibble(\n",
    "  method_name =names(data.frame(train_data[[\"errors\"]])), \n",
    "  column_avg = colSums(sub_tstdat)/nrow(sub_tstdat))\n",
    "\n",
    "# selector tool for relevant algorithms\n",
    "#apparent_algos <- sort(unlist(unique(mean_proba$column_avg)))\n",
    "\n",
    "# plotting the share of maximum for each method\n",
    "ggplot(mean_proba, \n",
    "       aes(x=method_name, y=column_avg))+\n",
    "  geom_bar(stat='identity',fill=colours)+\n",
    "  ggtitle(paste('Avg probability in each cls ', i))+\n",
    "  scale_x_discrete(limits=mean_proba$method_name)+\n",
    "  theme(\n",
    "    axis.text.x = element_text(angle = 90, size=20),\n",
    "    title = element_text(size=25))\n",
    "\n",
    "\n",
    "})\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "tags,comment,name,warning,-all",
   "main_language": "R",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
