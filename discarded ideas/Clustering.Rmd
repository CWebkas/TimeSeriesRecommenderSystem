---
title: "Clustering"
author: "Lukas Weixler"
date: "14 4 2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
pacman::p_load(tictoc, NbClust, tictoc, doParallel, rlist, tidyverse, ggfortify, data.table, BBmisc, xgboost, genieclust, BBmisc)
```



```{r}
kms <- kmeans(scale(all_available_feats), centers=3)
```


```{r}
hist(kms$cluster)
```
```{r}
tic()
kmsPca <- kmeans(pca_feats, centers = 3)
toc()
```


```{r}
hist(kmsPca$cluster)
```

```{r}
get_withinSS <- function(d, cluster){
  d <- stats::as.dist(d)
  cn <- max(cluster)
  clusterf <- as.factor(cluster)
  clusterl <- levels(clusterf)
  cnn <- length(clusterl)
  
  if (cn != cnn) {
    warning("cluster renumbered because maximum != number of clusters")
    for (i in 1:cnn) cluster[clusterf == clusterl[i]] <- i
    cn <- cnn
  }
  cwn <- cn
  # Compute total within sum of square
  dmat <- as.matrix(d)
  within.cluster.ss <- 0
  for (i in 1:cn) {
    cluster.size <- sum(cluster == i)
    di <- as.dist(dmat[cluster == i, cluster == i])
    within.cluster.ss <- within.cluster.ss + sum(di^2)/cluster.size
  }
  within.cluster.ss
}
```

```{r}
get_ave_sil_width <- function(d, cluster){
  if (!requireNamespace("cluster", quietly = TRUE)) {
    stop("cluster package needed for this function to work. Please install it.")
  }
  ss <- cluster::silhouette(cluster, d)
  mean(ss[, 3])
}
```


```{r}
feats_scaled <- scale(all_available_feats)


feats_normed <- normalize(all_available_feats, method = 'range', range=c(0,1))

```

### Run kmeans parallel
```{r}
cls <- rep(0, 8)

myCluster <- parallel::makeCluster(11, # number of cores to use
                         type = "PSOCK")
doParallel::registerDoParallel(myCluster)
tic()


clust_results <- foreach(i = c(2:30))%dopar%{
  
  cls <- kmeans(feats_scaled, i)
  return(cls)
  #v[i]<- get_withinSS(feats_scaled, cls$cluster)
  
}
toc()

stopCluster(myCluster)


wss <- lapply(clust_results, function(x){return(x$tot.withinss)})
df <- data.frame(matrix(unlist(wss), nrow=length(wss), byrow=TRUE))
autoplot(ts(df))
```




```{r}

cls <- rep(0, 8)

myCluster <- parallel::makeCluster(11, # number of cores to use
                         type = "PSOCK")
doParallel::registerDoParallel(myCluster)
tic()


clust_results <- foreach(i = c(2:30))%dopar%{
  
  cls <- kmeans(pca_feats, i)
  return(cls)
  #v[i]<- get_withinSS(feats_scaled, cls$cluster)
  
}
toc()

stopCluster(myCluster)


wss <- lapply(clust_results, function(x){return(x$tot.withinss)})
df <- data.frame(matrix(unlist(wss), nrow=length(wss), byrow=TRUE))
autoplot(ts(df))


```

```{r}
gaps <-cluster::clusGap(pca_feats, FUN=kmeans, nstart=20, K.max = 30, B=60)


```


```{r}
get_withinSS(feats_scaled, kmeans_results[[1]])
```




```{r}
feats_scaled <- scale(all_available_feats)
```

```{r}
library(factoextra)
library(FactoMineR)
library(tictoc)
```

```{r}
MyKmeansFUN <- function(x,k) list(cluster=kmeans(x, k))
```


### Use subsample for determining optimal number of clusters
```{r}
set.seed(31)
# function to compute total within-cluster sum of squares

samp_rows <- sample(nrow(pca_feats), 3326)

pca_feats_red <- pca_feats[samp_rows,c(1:80)]

tic()
fviz_nbclust(pca_feats_red, FUNcluster = kmeans, method = "gap_stat") 
toc()
```

```{r}
set.seed(3200)
# function to compute total within-cluster sum of squares

samp_rows <- sample(nrow(pca_feats), 100)

pca_feats_red <- pca_feats[samp_rows,c(1:80)]

tic()
fviz_nbclust(pca_feats_red, kmeans, method = "wss") 
toc()
```
```{r}
set.seed(2021)
# function to compute total within-cluster sum of squares

samp_rows <- sample(nrow(pca_feats), 1000)

pca_feats_red <- pca_feats[samp_rows,c(1:80)]

tic()
fviz_nbclust(pca_feats_red, kmeans, method = "wss") 
toc()
```
```{r}
set.seed(1234)
# function to compute total within-cluster sum of squares

samp_rows <- sample(nrow(pca_feats), 3326)

pca_feats_red <- pca_feats[samp_rows,]

tic()
fviz_nbclust(pca_feats_red, kmeans, method = "wss") 
toc()
```

```{r}

feats_scaled <- scale(all_available_feats)

feats_normed <- normalize(all_available_feats)

samp_rows <- sample(nrow(feats_scaled), 1000)

pca_feats_red <- feats_scaled[samp_rows,c(1:80)]

tic()
res.nbclust <- NbClust(pca_feats_red, distance = "euclidean",
                  min.nc = 2, max.nc = 9, 
                  method = "complete", index ="all")
toc()

samp_rows <- sample(nrow(feats_scaled), 1000)

pca_feats_red <- feats_scaled[samp_rows,c(1:80)]

tic()
res.nbclust <- NbClust(pca_feats_red, distance = "euclidean",
                  min.nc = 2, max.nc = 9, 
                  method = "complete", index ="all")
toc()

samp_rows <- sample(nrow(feats_scaled), 2000)

pca_feats_red <- feats_scaled[samp_rows,c(1:80)]

tic()
res.nbclust <- NbClust(pca_feats_red, distance = "euclidean",
                  min.nc = 2, max.nc = 9, 
                  method = "complete", index ="all")
toc()

samp_rows <- sample(nrow(feats_scaled), 3000)

pca_feats_red <- feats_scaled[samp_rows,c(1:80)]

tic()
res.nbclust <- NbClust(pca_feats_red, distance = "euclidean",
                  min.nc = 2, max.nc = 9, 
                  method = "complete", index ="all")
toc()
```

### Hierarchical Clustering

```{r}
cls <- genieclust::gclust(pc_90_hynd, gini_threshold = 1)


plot(cls)
```

```{r}

```
```{r}

out <- cutree(cls, h=0.8)

barnr <- max(unique(out))
hist(out, breaks=barnr)

```



```{r}


cls <- genieclust::gclust(feats_scaled)

```


```{r}
plot(cls)
```


```{r}
out <- cutree(cls, k=6)
```

```{r}
hist(out)
```
### Varying Parameters
```{r}
cls <- genieclust::gclust(feats_scaled, gini_threshold = 0.1)

```

```{r}
plot(cls)
```
```{r}
out2b <- cutree(cls, h=2.2)
```

```{r}
out3 <- cutree(cls, h=3)
```



### Distribution over clusters
```{r}

barnr <- max(unique(out2b))
hist(out2b, breaks=barnr)
```


```{r}
out2 <- cutree(cls, 33)
```

```{r}
hist(out2)
hist(out3)
```


### Prepare Splitting
```{r}
feats_scaled <- data.frame(feats_scaled)

# Sample for a split into 80 20
smp <- sample(nrow(feats_scaled), round(nrow(feats_scaled)*0.2))
```

### Split feats and labels into train and test datasets
```{r}

# test
feats_test <- feats_scaled[c(smp),]
labs_test <- out2[c(smp)]


# train
feats_train <- feats_scaled[-c(smp),]
labs_train <- out2[-c(smp)]



```

```{r}
labs_train <- as.factor(labs_train-1)
```


```{r}
feats_test <- data.matrix(feats_test)

feats_train <- data.matrix(feats_train)


labs_test <- as.factor(labs_test-1)
```


```{r}
bst <- xgboost(data = feats_train, label = labs_train, nthread = 11, nrounds = 10, max.depth=50, eta = 0.2, objective="multi:softprob",   eval_metric="mlogloss",
  num_class=34)
pred <- predict(bst, feats_test)
```

```{r}
num_class = length(levels(labs_train))
params = list(
  booster="gbtree",
  eta=0.2,
  max_depth=2,
  gamma=3,
  subsample=0.75,
  colsample_bytree=1,
  objective="multi:softprob",
  eval_metric="mlogloss",
  num_class=num_class+1,
  labels=labs_train
)
```

```{r}


dtrain <- xgb.DMatrix(feats_train,label=labs_train)

dtest <- xgb.DMatrix(feats_test,label=labs_test)

feats_train <- xgb.DMatrix(feats_train)
labs_train <- xgb.DMatrix(mat)

feats_test <- xgb.DMatrix(feats_test)
```


```{r}
xgb.fit=xgb.train(
  params=params,
  data=dtrain,
  nrounds=5,
  nthreads=11,
  watchlist=list(val1=dtrain,val2=dtest),
  verbose=0
)
```


```{r}
xgb.pred = predict(xgb.fit,feats_test,reshape=T)
```

```{r}
xgb.pred = as.data.frame(xgb.pred)
```

```{r}
mc <- max.col(xgb.pred)
```

```{r}

max(unique(mc))
```
```{r}
xgb.pred$prediction = max.col(xgb.pred)
xgb.pred$label = labs_test+1
```

```{r}
result = sum(xgb.pred$prediction==xgb.pred$label)/nrow(xgb.pred)
print(paste("Final Accuracy =",sprintf("%1.2f%%", 100*result)))
```


```{r}
sum(round(pred) == labs_test)/length(labs_test)
```


### Run DBSCAN in R




```{r}
db <- fpc::dbscan(pc_95_py, eps=0.1, MinPts = 50, scale = FALSE, 
       method = c("hybrid", "raw", "dist"))
```

```{r}
dbs <- dbscan::dbscan(feats_normed, eps = 1, minPts = 100)
```

```{r}
hist(dbs$cluster, breaks=12)
```

```{r}
dbs <- dbscan::dbscan(normed_hynd, eps = .5, minPts = 100)
```

```{r}
hist(dbs$cluster, breaks=12)
```


```{r}
hdbs <- dbscan::hdbscan(normed_hynd, 900)
```


```{r}
hist(hdbs$cluster, breaks=50)
```
```{r}
hdbs <- dbscan::hdbscan(normed_hynd, 800)

hist(hdbs$cluster, breaks=50)
```


```{r}
hdbs <- dbscan::hdbscan(normed_hynd, 700)

hist(hdbs$cluster, breaks=50)
```


```{r}
hdbs <- dbscan::hdbscan(normed_hynd, 600)

hist(hdbs$cluster, breaks=50)
```

```{r}

```

```{r}
opt_cls <- dbscan::optics(pc_90_hynd, eps = 1, minPts = 100)
plot(opt_cls)
```

```{r}
# Cut down the jumper
out = dbscan::extractXi(opt_cls, xi= 0.1)
```


```{r}
opt_cls <- dbscan::optics(normed_hynd, eps = 1, minPts = 100)
plot(opt_cls)
```


```{r}
opt_cls_all <- dbscan::optics(pc_95_py, eps = 10, minPts = 100)

plot(opt_cls_all)
```

```{r}
opt_cls_all <- dbscan::optics(feats_normed, eps =1, minPts = 100)

plot(opt_cls_all)
```


```{r}
out = dbscan::extractXi(opt_cls_all, xi= 0.9)
```


```{r}
res <- dbscan::optics_cut(opt_cls_all, eps_cl = .4)
res
```
```{r}
??optics_cut
```


```{r}
plot(db, fe)
```

