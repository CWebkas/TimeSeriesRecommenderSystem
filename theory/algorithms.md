| Method Name                     | Explanation                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | Implications                                                                                                                                                                        | Features to be checked                                                                                                                                                                                                                                                                          |
| ------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Naive                           | The simplest variant for a forecast model is the naive prediction. Here, the last value of the time series is taken and continued as a horizontal line for the desired forecast horizon. Despite its simplicity, this model fulfills several important functions: First, it is needed as a benchmark - only if the upcoming, significantly more complex methods deliver better forecasts is their use justified. Furthermore, despite - or even because of - its simplicity, the naive approach is considered to be a useful method for predicting unstructured time series. SOURCE. Naive predictions are determined in this paper with the help of the naive function from the package forecast of Hyndman and Khandakar in R.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | Unstructured TS has good naive performance<br>TS with mostly horizontal slope has good naive performance                                                                            | Peak of large support n (time series with many local maxima within a large range)<br>Concentration of the index mass to the left of the time series ,Flat Spots (long phases of horizontal slope)                                                                                               |
| Seasonal Naive                  | The same approach as in the naive model can also be used for seasonal time series: The last seasonal period is retained and continued for the desired forecast horizon. If no seasonal structure is available, a naive model according to the above approach is applied. In many cases, this results in identical error values for naive and seasonally naive forecasts. Seasonal naive predictions are determined in this paper with the help of the snaive function from the package forecast of Hyndman and Khandakar in R.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | Seasonal ts has better performance<br>TS with regular amplitude to positive or negative could have good snaive performance<br>Reoccuring patterns are preferred<br>Similar as Naive | Number of peaks of at least support is important<br>Skewness<br>Permutation Entropy<br>Change quantiles                                                                                                                                                                                         |
| Random Walk with Drift          | Another simple model for forecasting time series is random walk with drift. Here, a future value is forecast with its first lag Y\_t-1 added by a constant c and the normal iid error Z\_t. This leads to the classical random walk, based on the iid distribution with the additional drift component, represented by c. Thus, over a given period of time, the time series drifts in a negative direction when c is negative and in a positive direction when c is positive. In the present work, the rwf function from the forecast package is used for this purpose, whose drift parameter is activated by default. Thus, for time series with trend, we can assume a greater accuracy than for non-trending time series.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | Trending vs non trending time series - first will have better performance                                                                                                           | Regression- type features like agg\_linear trend or linear\_trend<br>First location of Maximum comes notably early or late<br>First location of Minimum comes notably early or late                                                                                                             |
| Theta Method                    | As a frequently used algorithm in the context of prediction competitions, the theta method of Assimakopoulos and Nikolopoulos (2000) is also to be included in this recommender system. Essentially, after a seasonal adjustment, several time series - so-called theta lines - are generated from the local curvature of the time series using exponnential smoothing. Each of these lines is extrapolated separately and the corresponding forecasts are then combined. Hyndman and Billah (2003) showed that this method is equivalent to simple exponnential smoothing with drift and thus a special variant of ETS. This matter was subject to discussion as in Nikolopoulos (Supply chain) and therefore the implication on the right will be investigated in this paper with special attention.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | Good theta performance implies good ETS performance<br>For linear TS we see high agg\_linear\_trend as indicator for good thetaf performance                                        | Regression- type features like agg\_linear trend or linear\_trend                                                                                                                                                                                                                               |
| Auto ARIMA                      | In this work, Arima models are generated using the auto.arima function from the forecast package of Hyndman and Khandakar (2008). The combination of autoregression, differentiation and moving average, which characterizes Arima, can be determined with the help of three parameters: Here, p stands for the order of the autoregressive component, d for the degree of the first difference, and q for the order of the moving average component. These parameters can either be determined by the user, or - as in the case of auto.arima - they are estimated by the algorithm itself based on the available values of the time series.<br><br>The algorithm for determining the parameters is described in detail in Hyndman and Khandakar (2008). Essentially, the optimal combination of parameters is determined using a set of candidate values based on Akaike's information criterion (AIC) for moving average, maximum likelihood estimates (MLE) for autoregression, and unit root tests for differencing. To determine the estimation accuracy, a score is formed using the accuracy measures mean absolute error (MAE), mean absolute percentage error (MAPE), mean absolute scaled error (MASE) and root mean square error (RMSE), according to which the candidate values are evaluated and selected.<br> | Stationary time series are expected to obtain more precise forecasts in ARIMA relative to ETS                                                                                       | Difficult to distinguish with one clear criterion from all other methods since autocorrelation, autoregression and differentation play key roles. Instead focus on effects of stationarity in relation to ETS<br>Reoccuring Datapoints (Stationarity)<br>Augmented Dickey Fuller (Stationarity) |
| Auto Exponnential Smoothing-ETS | The basis for predictions based on exponnential smoothing are weighted averages of observations from the past. There is a set of different ETS algorithms, which can be used depending on the available time series. Characteristic for the usability of a method are the two components trend (none, additive, additive damped) and seasonality (none, additive and multiplicative).  A third componnent (Error) is added in order to distinguish between additive and additive damped states. Therefore three parameters are used for ETS: error, trend and seasonality.<br><br>As with auto.arima, an optimal parameter combination for this method can be found using the ets function in the Forecast package of Hyndman and Khandakar (2008). Again, the AIC is used to select the optimal model for a given time series. For the case of ETS the formula on the right with L as the likelihood of the model and k the total number of parameters is used. The best method for a given time series is therefore determined using its log likelihood for the respective parameter combinations.                                                                                                                                                                                                                         | Stationary time series are expected to obtain more precise forecasts in ARIMA relative to ETS<br>Interval changes can be dampened through the ETS parameters.                       | Agg linear trend<br>Y\_fft<br>Angle of the fourier transform                                                                                                                                                                                                                                    |
| STLM                            | While seasonality is accounted for in a variety of the models already listed, especially in the case of financial time series, we may also encounter seasonality at multiple levels - for example, at weekly and monthly frequency. Time series with multiple seasonality can be decomposed into their different seasonal components in Hyndman and Khandagar's package using the STL function. Finally, a forecast can be formed from these seasonally adjusted data. In this paper, the default model of the from the Hyndman's M4metalearning package - ARIMA - is used. Thus after adjusting for complex seasonality, an auto-arima forecast is generated.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | Good STLM performance implies good auto arima performance<br>Seasonality might indicate good STLM performance<br>Multiple seasonality might result in amplitude patterns            | permutation entropy<br>change quantiles<br>amplitude features (fourier and wavelet)<br>                                                                                                                                                                                                         |
| Neural Network TS Forecasts     | Another commonly used method for modeling time series is neural network autoregression. The core idea is that a neural network with input layer, hidden layer and output layer predicts the next step based on the values of the past. Here, the lagged values of the time series are used as input to a neural network. The number of non-seasonal lags is additionally specified with , where 1 is set as the default value.  This default setting is kept in the present project. The hidden layer is automatically assigned a number of nodes - half of the input nodes. Since we predict 12 steps in this study, the nnetar function is applied recursively - a correspondingly long runtime is the consequence. In this work, nnetar from the forecast package by Hyndman and Kandakhar is used. Even though this algorithm is generally considered to have high accuracy, its performance may be relatively weak due to the many relatively short time series used in this work.                                                                                                                                                                                                                                                                                                                                      | Complex variance structures and long ts<br>Low overall performance due to short data is expected in this study                                                                      |                                                                                                                                                                                                                                                                                                 |
